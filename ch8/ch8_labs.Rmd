---
title: "Chapter 8 Labs: Tree-Based Methods"
output: 
    github_document: 
        toc: true
---

```{r setup, include = F}
# Set Chunk Options
knitr::opts_chunk$set(echo = TRUE)
 
# Load Packages
if(!require(tidyverse))     {install.packages("tidyverse")};    library(tidyverse)
if(!require(knitr))         {install.packages("knitr")};        library(knitr)
if(!require(ISLR2))         {install.packages("ISLR2")};        library(ISLR2)
if(!require(tree))          {install.packages("tree")};         library(tree)
if(!require(randomForest))  {install.packages("randomForest")}; library(randomForest)
if(!require(gbm))           {install.packages("gbm")};          library(gbm)
if(!require(BART))          {install.packages("BART")};         library(BART)
```

## Fitting Classification Trees

We use the `tree` library to construct classification and regression trees. We'll make a new categorical variable in the `Carseats` data set by recoding the `Sales` variable as being `High` or not. We'll then construct a classification tree to predict whether the sales are high or not  from everything except sales.

```{r}
Carseats$High <- factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
tree_carseats <- tree(High ~ . - Sales, data = Carseats)
summary(tree_carseats)
```

We can also plot the tree using the `plot()` function: 
```{r}
plot(tree_carseats)
text(tree_carseats, pretty = 0)
```

The summary gives us the training error rather than the test error, so we need to manually calculate the test error using the `predict()` function. 

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats_test <- Carseats[-train, ]
High_test <- Carseats$High[-train]

tree_carseats <- tree(High ~ . - Sales, data = Carseats, subset = train)
tree_pred <- predict(tree_carseats, Carseats_test, type = "class")
table(tree_pred, High_test)
mean(tree_pred == High_test)
```

So our classification tree has a prediction accuracy of `r 100*mean(tree_pred == High_test)`%. 

We can now prune the tree. We use the function `cv.tree()` to perform cross-validation to determine the optimal level of complexity. We use `FUN = prune.misclass` to indicate that we want the classification error rate to guide the process rather than the deviance. 

The `size` variable represents the number of terminal nodes in each tree, and the `k` variable corresponds to the complexity parameter $\alpha$ from the chapter. 

```{r}
set.seed(7)
cv_carseats <- cv.tree(tree_carseats, FUN = prune.misclass)
names(cv_carseats)
cv_carseats
```

We can notice that the tree with 9 terminal nodes has only 74 cross-validation errors (given by `dev`). 
```{r}
par(mfrow = c(1, 2))
plot(cv_carseats$size, cv_carseats$dev, type = "b")
plot(cv_carseats$k, cv_carseats$dev, type = "b")
```

We can now prune the tree so as to only have nine terminal nodes: 
```{r}
prune_carseats <- prune.misclass(tree_carseats, best = 9)
plot(prune_carseats)
text(prune_carseats, pretty = 0)
```

We'll now see how well it does on the test data set. 
```{r}
tree_pred <- predict(prune_carseats, Carseats_test, 
                     type = "class")
table(tree_pred, High_test)
mean(tree_pred == High_test)
```

We now have a `r 100*mean(tree_pred == High_test)`% classification accuracy, which is slightly improved over our $T_0$ tree and MUCH more interpretable. 


## Fitting Regression Trees

We'll fit a regression tree to the `Boston` data set. 
```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree_boston <- tree(medv ~ ., Boston, subset = train)
summary(tree_boston)
```

We can notice that only 4 of the variables were used in constructing the regression tree, hence performing variable selection. The deviance is just the sum of squared errors (SSE) for the tree. 
```{r}
plot(tree_boston)
text(tree_boston, pretty = 0)
```

We can now use `cv.tree()` to see if pruning the tree will improve the performance. 
```{r}
cv_boston <- cv.tree(tree_boston)
plot(cv_boston$size, cv_boston$dev, type = "b")
```
In this case, the most complex tree yields the lowest cross-validation deviance. If we wanted to prune the tree, though, we could do so using the `prune.tree()` function: 
```{r}
prune_boston <- prune.tree(tree_boston, best = 5)
plot(prune_boston)
text(prune_boston, pretty = 0)
```

Let's use the unpruned tree to make predictions on the test set: 
```{r}
yhat <- predict(tree_boston, newdata = Boston[-train, ])
boston_test <- Boston[-train, "medv"]
plot(yhat, boston_test)
abline(0, 1)
mean((yhat - boston_test)^2)
```

the RMSE then, is around 5.941, meaning that test predictions are (on average) within approximately \$5,941 of the true median home value for the census tract. 

## Bagging and Random Forests

We use the `randomForest` package to use bagging and random forests. Because bagging is a special case of random forests, to use bagging we simply let $m=p$. 

```{r}
set.seed(1)
bag_boston <- randomForest(medv ~ ., data = Boston, 
                           subset = train, 
                           mtry = 12, 
                           importance = TRUE)

bag_boston
```

By default, 500 trees are grown. How well does this perform on the test set? 
```{r}
yhat_bag <- predict(bag_boston, newdata = Boston[-train, ])
plot(yhat_bag, boston_test)
abline(0, 1)
mean((yhat_bag - boston_test)^2)
```

Our test MSE is about two-thirds that of the single tree, a good improvement. We can change the number of trees grown by `randomForest()` using the `ntree` argument: 
```{r}
bag_boston <- randomForest(medv ~ ., data = Boston, 
                           subset = train, 
                           mtry = 12, ntree = 25)
yhat_bag <- predict(bag_boston, newdata = Boston[-train, ])
mean((yhat_bag - boston_test)^2)
```
This is a slightly higher MSE than we got with 500 trees, which makes sense. 

We'll now grow a random forest, using `mtry = 6`. By default, `randomForest()` will use $p/3$ variables for regression trees, and $\sqrt{p}$ for classification trees. 
```{r}
set.seed(1)
rf_boston <- randomForest(medv ~ ., data = Boston, 
                          subset = train, mtry = 6, 
                          importance = TRUE)
yhat_rf <- predict(rf_boston, newdata = Boston[-train, ])
mean((yhat_rf - boston_test)^2)
```

We can now try with $m=5$ to see how they compare: 
```{r}
set.seed(1)
rf_boston <- randomForest(medv ~ ., data = Boston, 
                          subset = train, mtry = 5, 
                          importance = TRUE)
yhat_rf <- predict(rf_boston, newdata = Boston[-train, ])
mean((yhat_rf - boston_test)^2)
```
This yielded an improvement over the $m=6$ random forest, and a pretty good improvement over bagging as well. 

We can use the `importance()` function to view the importance of each variable: 
```{r}
importance(rf_boston)
```

A plot of this can be constructed using the `varImpPlot()` function: 
```{r}
varImpPlot(rf_boston)
```

The two variables reported in the variable importance data are: 

- Mean decrease in prediction accuracy on the out-of-bag samples when a given variable is permuted. 

- Measure of the total decrease in node impurity (or increase in node purity) that results from a split over that variable, averaged over all trees. With regression trees, node impurity is measured by the training RSS, and for classification it's measured by the deviance. 

Clearly, `rm` and `lstat` are the two most important variables. 

## Boosting

We use the `gbm` package for boosting, with the `gbm()` function. We'll fit boosted trees to the `Boston` data set as well. 

For regression, we use `distribution = "gaussian"`, whereas for a binary classification problem we would use `distribution = "bernoulli"`. `n.trees` controls how many trees, and `interaction.depth` controls the depth of each tree.

```{r}
set.seed(1)
boost_boston <- gbm(medv ~ ., data = Boston[train, ], 
                    distribution = "gaussian", 
                    n.trees = 5000, 
                    interaction.depth = 4)
summary(boost_boston)
```

This is a relative influence plot, but we can also provide partial dependence plots for the topmost two variables: 
```{r}
plot(boost_boston, i = "rm")
plot(boost_boston, i = "lstat")
```

As we would expect, median house prices increase with `rm` and decrease with `lstat`. 

We can calculate the test MSE for this as well using the `predict()` function: 
```{r}
yhat_boost <- predict(boost_boston, 
                      newdata = Boston[-train, ], 
                      n.trees = 5000)
mean((yhat_boost - boston_test)^2)
```

Now we'll try performing this with a different shrinkage parameter $\lambda$. The default value is 0.001, but this can be easily modified. Let's try $\lambda = 0.2$. 
```{r}
boost_boston <- gbm(medv ~ ., data = Boston[train, ], 
                    distribution = "gaussian", 
                    n.trees = 5000, 
                    interaction.depth = 4, 
                    shrinkage = 0.2, 
                    verbose = F)
yhat_boost <- predict(boost_boston, 
                      newdata = Boston[-train, ], 
                      n.trees = 5000)
mean((yhat_boost - boston_test)^2)
```
We can clearly see that changing the shrinkage parameter to 0.2 from 0.001 resulted in a much better test MSE, outperforming decision trees, bagging, and random forests. 

## Bayesian Additive Regression Trees

For Bayesian Additive Regression Trees, we will use the `BART` package, specifically the `gbart()` function. This is used for regression, whereas the `lbart()` and `pbart()` functions are used for binary, categorical outcomes. 

Unfortunately, the `gbart()` function does not take in the same syntax as `lm()` or the rest of the functions from this chapter, and we have to create train and test matrices. 
```{r}
x <- Boston[, 1:12]
y <- Boston[, "medv"]

xtrain <- x[train, ]
ytrain <- y[train]

xtest <- x[-train, ]
ytest <- y[-train]

set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```

Now we can compute the test error: 
```{r}
yhat_bart <- bartfit$yhat.test.mean
mean((ytest - yhat_bart)^2)
```
This yielded the lowest test error of all the techniques seen in this lab. 

We can now check how many times each variable appeared in teh collection of all the trees. 
```{r}
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
```